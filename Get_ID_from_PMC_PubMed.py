# -*- coding:utf-8 -*-
# ! usr/bin/env python3
"""
Created on 01/06/2021 21:54
@Author: XINZHI YAO
"""

import os

import json
import requests

import argparse

def getid(searchTerm, db='pubmed'):
    try:
        import xml.etree.cElementTree as ET
    except ImportError:
        import xml.etree.ElementTree as ET

    id_set = set()
    idlist = []
    query = searchTerm
    base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'
    url = base + "esearch.fcgi?db=" + db + "&term=" + query + "&retmode=json"
    re = requests.get(url)
    result = json.loads(re.text)
    total = result["esearchresult"]["count"]
    iternum = int(int(total) / 100000) + 1
    for i in range(iternum):
        retstart = 1 + 100000 * i
        url = base + "esearch.fcgi?db=" + db + "&term=" + query + "&retmode=json" + "&retmax=100000"
        re = requests.get(url)
        result = json.loads(re.text)
        idlist = result["esearchresult"]["idlist"]
        print(f'update: {len(idlist)}')
        id_set.update(idlist)


    return idlist


def get_id(token_file: str, save_path: str):

    """
    The input file can have one or more columns, and each row corresponds to a Term.
    Search PMID and PMC at the same time
    Input: Term MeSH
    Output: Term    PMC/PMID    Source
    """
    print(f'TokenFile: {token_file}')
    with open(token_file, encoding='utf-8') as f:
        #f.readline()
        for line in f:
            total_id = set()

            l = line.strip().split('\t')
            token_set = set(l)
            print(f'Start: {token_set}.')

            for token in token_set:
                print(f'Token: {token}.')
                pmid_set = getid(token, 'pubmed')
                pmc_set = getid(token, 'pmc')

                for pmid in pmid_set:
                    total_id.add((pmid, 'PMID'))
                for pmc in pmc_set:
                    total_id.add((pmc, 'PMC'))
                print(f'Token PMC/PMID Count: {len(total_id)}')

            save_file = f'{save_path}/{l[0]}.pmc.pmid.txt'
            print(f'Save file: {save_file}')
            with open(save_file, 'w', encoding='utf-8') as wf:
                wf.write('Term\tPMC/PMID\tSource\n')
                for _id, source in total_id:
                    wf.write(f'{l[0]}\t{_id}\t{source}\n')
            print(f'{l[0]} save done, save count: {len(total_id):,}.')


def extract_pmc_pmid(input_path: str, save_file: str):
    """
    Extract pmid and pmc from all files generated by the get_id
    function to generate an id file with only one column for Pubtator retrieval
    Input: Term    PMC/PMID    Source
    """
    file_list = os.listdir(input_path)

    total_id_set = set()
    for _file in file_list:
        file_path = os.path.join(input_path, _file)
        with open(file_path) as f:
            f.readline()
            for line in f:
                _id = line.strip().split('\t')[1]
                total_id_set.add(_id)

    print(f'{len(file_list)} files, {len(total_id_set):,} PMC/PMID.')
    with open(save_file, 'w', encoding='utf-8') as wf:
        for _id in total_id_set:
            wf.write(f'{_id}\n')
    print(f'{save_file} save done.')


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Lit-GWAS Bayes model.')

    parser.add_argument('-if', dest='id_file',
                        default='../data/APOE_dir/APOE.keyword.txt',
                        help='default: ../data/APOE_dir/APOE.keyword.txt')

    parser.add_argument('-sp', dest='pmid_save_path',
                        default='../data/APOE_dir/APOE_id_info',
                        help='default: ../data/APOE_dir/APOE_id_info')

    args = parser.parse_args()

    # id_file = '../data/APOE_dir/APOE.keyword.txt'
    # pmid_save_path = '../data/APOE_dir/APOE_id_info'

    if not os.path.exists(args.pmid_save_path):
        os.mkdir(args.pmid_save_path)

    print('get id')
    get_id(args.id_file, args.pmid_save_path)


